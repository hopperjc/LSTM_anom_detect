{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Criar pastas de saída\n",
    "os.makedirs('processed', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "# 1. Fetch dataset Metro Interstate Traffic Volume (id=492)\n",
    "print('Carregando dataset...')\n",
    "dataset = fetch_ucirepo(id=492)\n",
    "df = dataset.data.features.copy()\n",
    "df['traffic_volume'] = dataset.data.targets.astype(int)\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.sort_values('date_time', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 2. Salvar CSV completo\n",
    "df.to_csv('./processed/traffic_full.csv', index=False)\n",
    "print('CSV completo salvo em processed/traffic_full.csv')\n",
    "\n",
    "# 3. EDA: plot série completa\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df['date_time'], df['traffic_volume'])\n",
    "plt.title('Traffic Volume Over Time')\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Volume')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/traffic_full.png')\n",
    "plt.close()\n",
    "print('Plot completo salvo em plots/traffic_full.png')\n",
    "\n",
    "# 4. Plot por ano\n",
    "for year, grp in df.groupby(df['date_time'].dt.year):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(grp['date_time'], grp['traffic_volume'])\n",
    "    plt.title(f'Traffic Volume in {year}')\n",
    "    plt.xlabel('Date Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.tight_layout()\n",
    "    path = f'./plots/traffic_{year}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    print(f'Plot {year} salvo em {path}')\n",
    "\n",
    "# 4b. Plot por mês (ano-mês)\n",
    "for (year, month), grp in df.groupby([df['date_time'].dt.year, df['date_time'].dt.month]):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(grp['date_time'], grp['traffic_volume'])\n",
    "    plt.title(f'Traffic Volume in {year}-{month:02d}')\n",
    "    plt.xlabel('Date Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.tight_layout()\n",
    "    path = f'./plots/traffic_{year}_{month:02d}.png'\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "    print(f'Plot {year}-{month:02d} salvo em {path}')\n",
    "\n",
    "# 5. Anomalias via Z-score univariada (threshold=2)\n",
    "traffic = df['traffic_volume'].values\n",
    "z_scores = np.abs(stats.zscore(traffic))\n",
    "thresh = 2\n",
    "anomaly_df = df[z_scores > thresh].copy()\n",
    "anomaly_df.to_csv('./processed/anomalies_zscore.csv', index=False)\n",
    "print('Anomalias Z-score salvas em processed/anomalies_zscore.csv')\n",
    "\n",
    "# 6. Pré-processamento multivariado\n",
    "features = ['holiday','temp','rain_1h','snow_1h','clouds_all','weather_main']\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), ['holiday','weather_main']),\n",
    "    ('num', MinMaxScaler(), ['temp','rain_1h','snow_1h','clouds_all'])\n",
    "])\n",
    "X_all = preprocessor.fit_transform(df[features])\n",
    "y_all = df['traffic_volume'].values\n",
    "\n",
    "# Montar DataFrame para CSV preprocessed\n",
    "cols = preprocessor.get_feature_names_out()\n",
    "df_proc = pd.DataFrame(X_all, columns=cols)\n",
    "df_proc['traffic_volume'] = y_all\n",
    "df_proc['date_time'] = df['date_time'].values\n",
    "df_proc.to_csv('./processed/preprocessed_features.csv', index=False)\n",
    "print('Recursos pré-processados salvos em processed/preprocessed_features.csv')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Carregar dados pré-processados\n",
    "df = pd.read_csv('processed/preprocessed_features.csv')\n",
    "window = 24\n",
    "\n",
    "# Extrair datas, features e target\n",
    "dates = pd.to_datetime(df['date_time'])\n",
    "y = df['traffic_volume'].values\n",
    "X = df.drop(columns=['traffic_volume','date_time']).values\n",
    "\n",
    "# Função para criar sequências\n",
    "def create_sequences(X, y, window):\n",
    "    seq_X, seq_y = [], []\n",
    "    for i in range(len(X) - window):\n",
    "        seq_X.append(X[i:i+window])\n",
    "        seq_y.append(y[i+window])\n",
    "    return np.array(seq_X), np.array(seq_y)\n",
    "\n",
    "# Criar sequências e dividir cronologicamente\n",
    "total_seq, total_y = create_sequences(X, y, window)\n",
    "split_idx = int(len(total_seq) * 0.7)\n",
    "X_train, y_train = total_seq[:split_idx], total_y[:split_idx]\n",
    "X_test,  y_test  = total_seq[split_idx:], total_y[split_idx:]\n",
    "\n",
    "# Converter para tensores\n",
    "torch.manual_seed(42)\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "X_test_t  = torch.from_numpy(X_test).float()\n",
    "y_test_t  = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Modelo LSTM\n",
    "torch.manual_seed(42)\n",
    "class LSTMAnomalyDetect(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden=64, n_layers=2, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden,\n",
    "                            num_layers=n_layers, dropout=drop, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "model = LSTMAnomalyDetect(n_features=X_train.shape[2]).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Treino com early stopping\n",
    "best_loss, patience, trials = np.inf, 5, 0\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    val_pred = model(X_train_t.to(device)).detach().cpu().numpy().flatten()\n",
    "    val_loss = mean_squared_error(y_train, val_pred)\n",
    "    print(f\"Epoch {epoch}: train MSE={np.mean(losses):.4f}, valid MSE={val_loss:.4f}\")\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss; trials = 0\n",
    "        torch.save(model.state_dict(), 'best_lstm.pt')\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# Carregar melhor modelo e avaliar\n",
    "model.load_state_dict(torch.load('best_lstm.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Previsões e métricas\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train_t.to(device)).cpu().numpy().flatten()\n",
    "    y_test_pred  = model(X_test_t.to(device)).cpu().numpy().flatten()\n",
    "\n",
    "# Métricas de regressão\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mse  = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "print(f\"TRAIN -> MSE: {train_mse:.2f}, MAE: {train_mae:.2f}\")\n",
    "print(f\" TEST -> MSE: {test_mse:.2f}, MAE: {test_mae:.2f}\")\n",
    "\n",
    "# Threshold para detecção de anomalias a partir do erro de treino\n",
    "train_errors = np.abs(y_train - y_train_pred)\n",
    "thresh = train_errors.mean() + 2 * train_errors.std()\n",
    "print(f\"Threshold de anomalia (treino): {thresh:.2f}\")\n",
    "\n",
    "# Detectar anomalias no conjunto de teste\n",
    "test_errors = np.abs(y_test - y_test_pred)\n",
    "idx_anom = np.where(test_errors > thresh)[0]\n",
    "print(f\"Anomalias detectadas no TEST: {len(idx_anom)} de {len(y_test)} pontos\")\n",
    "\n",
    "# Plot de predição vs real e anomalias\n",
    "times_test = dates.iloc[window+split_idx:].reset_index(drop=True)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(times_test, y_test, label='True')\n",
    "plt.plot(times_test, y_test_pred, label='Pred')\n",
    "plt.scatter(times_test.iloc[idx_anom], y_test[idx_anom], c='red', s=10, label='Anomalia')\n",
    "plt.title('Detecção de Anomalias (Teste)')\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig('plots/anomaly_detection_test.png')\n",
    "plt.close()\n",
    "print('Plot de anomalias de teste salvo em plots/anomaly_detection_test.png')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurações\n",
    "dir_results = 'results'\n",
    "dir_plots = 'plots'\n",
    "s=os.makedirs(dir_results, exist_ok=True)\n",
    "os.makedirs(dir_plots, exist_ok=True)\n",
    "\n",
    "# 1) Carregar dados brutos de tráfego\n",
    "# Usamos o CSV completo para manter timestamp e volume\n",
    "df = pd.read_csv('processed/traffic_full.csv')\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "\n",
    "def split_train_test(df, train_frac=0.7):\n",
    "    split_time = df['date_time'].quantile(train_frac)\n",
    "    train = df[df['date_time'] <= split_time]\n",
    "    test  = df[df['date_time'] >  split_time]\n",
    "    return train, test\n",
    "\n",
    "train_df, test_df = split_train_test(df)\n",
    "\n",
    "# 2) Calcular baseline por hora (média e desvio)\n",
    "hour_stats = train_df.groupby('hour')['traffic_volume'] \\\n",
    "    .agg(['mean','std']).reset_index().rename(columns={'mean':'hour_mean','std':'hour_std'})\n",
    "\n",
    "# 3) Identificar anomalias no teste\n",
    "# Mesclar stats no test\n",
    "test_df = test_df.merge(hour_stats, how='left', on='hour')\n",
    "# Definir threshold k sigma (k=2)\n",
    "test_df['threshold'] = test_df['hour_mean'] + 2 * test_df['hour_std']\n",
    "# Flag de anomalia\n",
    "anoms = test_df[test_df['traffic_volume'] > test_df['threshold']].copy()\n",
    "\n",
    "# 4) Salvar resultados\n",
    "train_df.to_csv(os.path.join(dir_results,'hourly_baseline_train.csv'), index=False)\n",
    "anoms.to_csv(os.path.join(dir_results,'anomalies_by_hour_threshold.csv'), index=False)\n",
    "print(f\"Anomalias detectadas: {len(anoms)} de {len(test_df)} pontos no teste\")\n",
    "\n",
    "# 5) Gráfico: distribuição de anomalias por hora do dia\n",
    "hour_counts = anoms['hour'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(x=hour_counts.index, y=hour_counts.values)\n",
    "plt.title('Anomalias por Hora do Dia (Threshold por Hora)')\n",
    "plt.xlabel('Hora')\n",
    "plt.ylabel('Número de Anomalias')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dir_plots,'anomalies_by_hour_threshold.png'))\n",
    "plt.close()\n",
    "\n",
    "# 6) Timeline das anomalias\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_df['date_time'], test_df['traffic_volume'], alpha=0.3, label='Traffic')\n",
    "plt.scatter(anoms['date_time'], anoms['traffic_volume'], color='red', s=10, label='Anomalias')\n",
    "plt.title('Anomalias pelo Threshold por Hora - Timeline')\n",
    "plt.xlabel('Date Time')\n",
    "plt.ylabel('Volume')\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(os.path.join(dir_plots,'anomalies_timeline_hour_threshold.png'))\n",
    "plt.close()\n",
    "\n",
    "print('Análises por hora concluídas.')\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurações\n",
    "dir_results = 'results'\n",
    "dir_plots = 'plots'\n",
    "window_sizes = [7, 14, 30]  # janelas em dias\n",
    "os.makedirs(dir_results, exist_ok=True)\n",
    "os.makedirs(dir_plots, exist_ok=True)\n",
    "\n",
    "# 1) Carregar dados brutos\n",
    "df = pd.read_csv('processed/traffic_full.csv')\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "df.sort_values('date_time', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Extrair hora\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "\n",
    "# 2) Para cada janela, calcular threshold móvel e gerar resultados\n",
    "for window in window_sizes:\n",
    "    df_copy = df.copy()\n",
    "    days = window  # número de dias para rolling\n",
    "    \n",
    "    # Rolling: precisamos de tantas observações quanto dias por hora\n",
    "    df_copy['rolling_mean'] = df_copy.groupby('hour')['traffic_volume'].transform(lambda x: x.shift().rolling(window=days, min_periods=3).mean())\n",
    "    df_copy['rolling_std'] = df_copy.groupby('hour')['traffic_volume'].transform(lambda x: x.shift().rolling(window=days, min_periods=3).std())\n",
    "\n",
    "    # Threshold e flag\n",
    "    df_copy['threshold'] = df_copy['rolling_mean'] + 2 * df_copy['rolling_std']\n",
    "    df_copy['is_anomaly'] = df_copy['traffic_volume'] > df_copy['threshold']\n",
    "    anoms = df_copy[df_copy['is_anomaly']].copy()\n",
    "    print(f\"Window {window} dias: {len(anoms)} anomalias de {len(df_copy)} pontos\")\n",
    "\n",
    "    # 3) Salvar CSV de anomalias\n",
    "    fname = f'rolling_{window}d_anomalies.csv'\n",
    "    anoms.to_csv(os.path.join(dir_results, fname), index=False)\n",
    "\n",
    "    # 4) Resumo por hora\n",
    "    summary = df_copy.groupby('hour').agg(\n",
    "    total=('traffic_volume','size'),anomalies=('is_anomaly','sum')).reset_index()\n",
    "    summary['anomaly_rate'] = summary['anomalies'] / summary['total']\n",
    "    sname = f'rolling_{window}d_summary.csv'\n",
    "    summary.to_csv(os.path.join(dir_results, sname), index=False)\n",
    "\n",
    "    # 5) Plot taxa de anomalias por hora\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.lineplot(x='hour', y='anomaly_rate', data=summary, marker='o')\n",
    "    plt.title(f'Taxa de Anomalias por Hora (Rolling {window} dias)')\n",
    "    plt.xlabel('Hora')\n",
    "    plt.ylabel('Taxa de Anomalias')\n",
    "    plt.tight_layout()\n",
    "    pname = f'rolling_{window}d_rate_by_hour.png'\n",
    "    plt.savefig(os.path.join(dir_plots, pname))\n",
    "    plt.close()\n",
    "\n",
    "    # 6) Timeline completo\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(df_copy['date_time'], df_copy['traffic_volume'], alpha=0.3)\n",
    "    plt.scatter(anoms['date_time'], anoms['traffic_volume'], color='red', s=8)\n",
    "    plt.title(f'Anomalias (Rolling {window} dias)')\n",
    "    plt.xlabel('Date Time')\n",
    "    plt.ylabel('Volume')\n",
    "    plt.tight_layout()\n",
    "    tname = f'rolling_{window}d_timeline.png'\n",
    "    plt.savefig(os.path.join(dir_plots, tname))\n",
    "    plt.close()\n",
    "\n",
    "print('Análises para janelas de 14 e 30 dias concluídas.')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Carregar previsões e verdadeiros\n",
    "df = pd.read_csv('results/full_test_set.csv')\n",
    "# Supondo colunas 'true' e 'pred' no conjunto de teste\n",
    "true = df['true'].values\n",
    "pred = df['pred'].values\n",
    "\n",
    "# Calcular métricas\n",
    "mse = mean_squared_error(true, pred)\n",
    "mae = mean_absolute_error(true, pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Exibir resultados\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Salvar em CSV\n",
    "metrics_df = pd.DataFrame({\n",
    "    'metric': ['MSE','MAE','RMSE'],\n",
    "    'value': [mse, mae, rmse]\n",
    "})\n",
    "metrics_df.to_csv('results/metrics.csv', index=False)\n",
    "print('Métricas salvas em results/metrics.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
